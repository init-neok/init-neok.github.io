@inproceedings{zheng2025reefknot,
  abbr      = {ACL-Findings},
  title     = {Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models},
  author    = {Zheng*, Kening and Chen*, Junkai and Yan, Yibo and Zou, Xin and Hu, Xuming},
  abstract  = {Hallucination issues continue to affect multimodal large language models (MLLMs), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations that require advanced reasoning. Current benchmarks for relation hallucinations lack detailed evaluation and effective mitigation, and their datasets often suffer from biases due to systematic annotation processes. To address these challenges, we introduce Reefknot, a comprehensive benchmark targeting relation hallucinations, comprising over 20,000 real-world samples. We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives, and construct a relation-based corpus using the Visual Genome scene graph dataset. Our comparative evaluation reveals significant limitations in current MLLMs' ability to handle relation hallucinations. Additionally, we propose a novel confidence-based mitigation strategy, which reduces the hallucination rate by an average of 9.75% across three datasets, including Reefknot. Our work offers valuable insights for achieving trustworthy multimodal intelligence.},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2025},
  publisher = {Association for Computational Linguistics},
  year      = {2025},
  pages     = {To appear},
  doi       = {10.18653/v1/2025.findings-acl.xxx},
  url       = {https://arxiv.org/abs/2408.09429},
  pdf       = {https://arxiv.org/pdf/2408.09429},
  code      = {https://github.com/JackChen-seu/Reefknot},
  selected  = {true}
}
@inproceedings{chen2025safeeraser,
  abbr      = {ACL-Findings},
  title     = {SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning},
  author    = {Chen, Junkai and Deng, Zhijie and Zheng, Kening and Yan, Yibo and Liu, Shuliang and Wu, PeiJun and Jiang, Peijie and Liu, Jia and Hu, Xuming},
  abstract  = {As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. Machine Unlearning (MU), as an effective strategy for forgetting specific knowledge in training data, has been widely used in privacy protection. However, MU for safety in MLLM has yet to be fully explored. To address this issue, we propose SAFEERASER, a safety unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs. We comprehensively evaluate unlearning methods from two perspectives: forget quality and model utility. Our findings show that existing MU methods struggle to maintain model performance while implementing the forget operation and often suffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to alleviate over-forgetting through decouple prompt during unlearning process. To quantitatively measure over-forgetting mitigated by PD Loss, we propose a new metric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate that combining PD Loss with existing unlearning methods can effectively prevent over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B and LLaVA-13B, while maintaining forget quality and model utility. Our code and dataset will be released upon acceptance. Warning: This paper contains examples of harmful language and images, and reader discretion is recommended.},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2025},
  publisher = {Association for Computational Linguistics},
  year      = {2025},
  pages     = {To appear},
  doi       = {10.18653/v1/2025.findings-acl.xxx},
  url       = {https://arxiv.org/abs/2502.12520},
  pdf       = {https://arxiv.org/pdf/2502.12520},
  selected  = {true}
}

@inproceedings{zou2025looktwice,
  abbr      = {ICML},
  title     = {Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models},
  author    = {Zou, Xin and Wang, Yizhou and Yan, Yibo and Lyu, Yuanhuiyi and Zheng, Kening and Huang, Sirui and Chen, Junkai and Jiang, Peijie and Liu, Jia and Tang, Chang and Hu, Xuming},
  abstract  = {Despite their impressive capabilities, multimodal large language models (MLLMs) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in MLLMs often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to "amnesia" about visual information. To address this issue, we propose MemVR, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. Following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the MLLM through Feed Forward Network (FFN) as "key-value memory" at the middle trigger layer. This "look-twice" mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. Comprehensive experimental evaluations demonstrate that MemVR significantly mitigates hallucination across various MLLMs and excels in general benchmarks without incurring additional time overhead.},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  publisher = {PMLR},
  year      = {2025},
  pages     = {To appear},
  doi       = {10.48550/arXiv.2410.03577},
  url       = {https://arxiv.org/abs/2410.03577},
  pdf       = {https://arxiv.org/pdf/2410.03577},
  code      = {https://github.com/1zhou-Wang/MemVR},
  selected  = {true}
}


@inproceedings{li2024refiner,
  abbr      = {EMNLP-Findings},
  title     = {Refiner: Restructure Retrieved Content Efficiently to Advance Question-Answering Capabilities},
  author    = {Li, Zhonghao and Hu, Xuming and Liu, Aiwei and Zheng, Kening and Huang, Sirui and Xiong, Hui},
  abstract  = {Large Language Models (LLMs) often hallucinate in knowledge-extensive tasks due to limited parametric knowledge. To mitigate this, Retrieval-Augmented Generation (RAG) uses external document chunks, but struggles with scattered key information ("lost-in-the-middle" syndrome). We propose Refiner, an end-to-end extract-and-restructure framework using a single decoder-only LLM to selectively extract and contextually restructure query-relevant contents. Experiments show Refiner significantly improves answer accuracy and reduces tokens by 80.5%, outperforming advanced RAG and compressing methods by 1.6-7.0% in multi-hop QA tasks. As a plug-and-play solution, Refiner integrates seamlessly with diverse RAG frameworks.},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  publisher = {Association for Computational Linguistics},
  year      = {2024},
  pages     = {8548--8572},
  doi       = {10.18653/v1/2024.findings-emnlp.500},
  url       = {http://dx.doi.org/10.18653/v1/2024.findings-emnlp.500},
  code      = {https://github.com/allen-li1231/refiner-rag},
  selected  = {true}
}
